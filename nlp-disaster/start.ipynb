{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a232a83c-f730-414b-9632-f55ef50ccbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import pipeline, metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2b61ed5f-3bcf-4a3b-83c5-44d5f3e0ec58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cb0577954e45f4b46f5b8c3163dd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c5bf15be-2fb7-4a53-adca-33f33432e806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length : 7613\n",
      "Test length : 3263\n"
     ]
    }
   ],
   "source": [
    "sample_path = \"./data/sample_submission.csv\"\n",
    "train_path = \"./data/train.csv\"\n",
    "test_path = \"./data/test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_sample = pd.read_csv(sample_path)\n",
    "\n",
    "print(f\"Train length : {len(df_train)}\")\n",
    "print(f\"Test length : {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d1516b40-60cd-4947-b870-8e5e5ad6c297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "32c94576-e491-48d4-a5df-021b36a21100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Birmingham', 'Est. September 2012 - Bristol', ...,\n",
       "       'Vancouver, Canada', 'London ', 'Lincoln'], dtype=object)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "10387571-54d3-4654-b19c-09a48a5586f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 1523)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2)\n",
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d381beb1-2642-4c40-b602-3a8e5a4839a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9efe0e91-562e-4b09-b358-130430e9b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train = list(df_train.target), list(df_train.text)\n",
    "y_val, x_val = list(df_val.target), list(df_val.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246ece0-77fc-44d5-b162-ba7a2daea469",
   "metadata": {},
   "source": [
    "## TFIDF and CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "adae7917-034d-400d-9346-dbe9a02702af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLoss(target, pred, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Use sklearn.metrics.log_loss built in function instead\n",
    "    \"\"\"\n",
    "    clip = np.clip(pred, epsilon, 1-epsilon)\n",
    "    N, M = pred.shape\n",
    "    y_onehot = np.zeros((N, M))\n",
    "    for i, val in enumerate(target):\n",
    "        y_onehot[i, val] = 1\n",
    "    logLoss = -1/N * np.sum(y_onehot*np.log(clip))\n",
    "    return logLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1997dabe-7328-4820-baa1-cb0598d58b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6090, 9229), (1523, 9229))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
    "                     analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                     ngram_range=(1,3), use_idf=1, smooth_idf=1,\n",
    "                     sublinear_tf=1, stop_words='english')\n",
    "\n",
    "tfv.fit(x_train + x_val) # semi-supervised learning\n",
    "xtrain_tfv = tfv.transform(x_train)\n",
    "xval_tfv = tfv.transform(x_val)\n",
    "xtrain_tfv.shape, xval_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b5cddf66-60bb-4b52-9077-edc9d9ea6ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss = 0.4941405696220364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, y_train)\n",
    "pred_tfv = clf.predict_proba(xval_tfv)\n",
    "print(f\"LogLoss = {LogLoss(y_val, pred_tfv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8d13113e-f510-40ee-8c71-1dafae296c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss = 0.4630015513684077\n"
     ]
    }
   ],
   "source": [
    "ctv = CountVectorizer(analyzer=\"word\", token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1,3), stop_words=\"english\")\n",
    "ctv.fit(x_train+x_val)\n",
    "xtrain_ctv = ctv.transform(x_train)\n",
    "xval_ctv = ctv.transform(x_val)\n",
    "\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, y_train)\n",
    "pred_ctv = clf.predict_proba(xval_ctv)\n",
    "print(f\"LogLoss = {LogLoss(y_val, pred_ctv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6bafe-f955-490c-85ed-b2996517cd08",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "286adfb9-877c-492f-8312-dc05a616d931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss = 0.48975354981090746\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, \n",
    "                        colsample_bytree=0.8, subsample=0.8,\n",
    "                        nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv, y_train)\n",
    "pred_xg = clf.predict_proba(xval_tfv)\n",
    "print(f\"LogLoss = {metrics.log_loss(y_val, pred_xg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11827db-51df-4be4-a2ec-d2ae8428c971",
   "metadata": {},
   "source": [
    "## Grid Search with SVD and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5b23709c-ece7-470a-b159-09700fad320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "99953f47-e66f-4c08-83cf-43093396ff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Best score: 0.5141388385839437\n",
      "Best parameters set: \n",
      "lr__C : 0.1\n",
      "svd__n_components : 180\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD()\n",
    "scl = StandardScaler()\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "        ('svd', svd),\n",
    "        ('slc', scl),\n",
    "        ('lr', lr_model),\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    'svd__n_components' : [120, 180],\n",
    "    'lr__C' : [0.1, 1.0, 10],\n",
    "}\n",
    "\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer, verbose=10, n_jobs=-1, refit=True, cv=2)\n",
    "model.fit(xtrain_tfv, y_train)\n",
    "\n",
    "print(f\"Best score: {-model.best_score_}\")\n",
    "print(\"Best parameters set: \")\n",
    "best_params = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(f\"{param_name} : {best_params[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a999e4ba-5599-49a3-8610-c78a4c36398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best score: 0.49018914761693233\n",
      "Best parameters set: \n",
      "nb__alpha : 1\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "        ('nb', nb_model),\n",
    "    ])\n",
    "\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer, verbose=10, n_jobs=-1, refit=True, cv=2)\n",
    "model.fit(xtrain_tfv, y_train)\n",
    "\n",
    "print(f\"Best score: {-model.best_score_}\")\n",
    "print(\"Best parameters set: \")\n",
    "best_params = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(f\"{param_name} : {best_params[param_name]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37c013-7f75-4e27-b49e-39f40359adad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Try to fine tuned BERT ---> failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "533e522d-3f8e-451f-a193-ce87efa2e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8ab47698-3fb8-4fb1-a34a-a0112ea43db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-80461f5a0f47ecd9\n",
      "Found cached dataset csv (/home/hainam/.cache/huggingface/datasets/csv/default-80461f5a0f47ecd9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03731203079223633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 45,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c4896e0adf4ec3862e274d38832948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2842d9be0ed29a04\n",
      "Found cached dataset csv (/home/hainam/.cache/huggingface/datasets/csv/default-2842d9be0ed29a04/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04125094413757324,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 45,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6b4fa75774429a9c085a78c7a455c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_dataset = load_dataset(\"csv\", data_files=train_path)\n",
    "test_dataset = load_dataset(\"csv\", data_files=test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2f48f019-9bfb-483a-bf1d-c2fa8fa73a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"train\"] = train_dataset[\"train\"].remove_columns(['id', 'keyword', 'location'])\n",
    "train_dataset[\"train\"] = train_dataset[\"train\"].rename_column(\"target\", \"label\")\n",
    "\n",
    "train_val_dataset = train_dataset['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d86951-38c1-4ab8-a537-868a6d959823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be453339-e7cb-4744-9c6f-62dfb4e584f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "encoded_dataset = train_val_dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c74dd138-fd86-4ef8-a0a2-dccbc004d312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "f1_metric = load_metric(\"f1\")\n",
    "metric_name = \"f1\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    label_names=[\"label\"],\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
