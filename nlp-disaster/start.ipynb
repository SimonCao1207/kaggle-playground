{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a232a83c-f730-414b-9632-f55ef50ccbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import pipeline, metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5bf15be-2fb7-4a53-adca-33f33432e806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length : 7613\n",
      "Test length : 3263\n"
     ]
    }
   ],
   "source": [
    "sample_path = \"./data/sample_submission.csv\"\n",
    "train_path = \"./data/train.csv\"\n",
    "test_path = \"./data/test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_sample = pd.read_csv(sample_path)\n",
    "\n",
    "print(f\"Train length : {len(df_train)}\")\n",
    "print(f\"Test length : {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1516b40-60cd-4947-b870-8e5e5ad6c297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32c94576-e491-48d4-a5df-021b36a21100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Birmingham', 'Est. September 2012 - Bristol', ...,\n",
       "       'Vancouver, Canada', 'London ', 'Lincoln'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10387571-54d3-4654-b19c-09a48a5586f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 1523)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2)\n",
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d381beb1-2642-4c40-b602-3a8e5a4839a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9efe0e91-562e-4b09-b358-130430e9b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train = list(df_train.target), list(df_train.text)\n",
    "y_val, x_val = list(df_val.target), list(df_val.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246ece0-77fc-44d5-b162-ba7a2daea469",
   "metadata": {},
   "source": [
    "## TFIDF and CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "adae7917-034d-400d-9346-dbe9a02702af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLoss(target, pred, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Use sklearn.metrics.log_loss built in function instead\n",
    "    \"\"\"\n",
    "    clip = np.clip(pred, epsilon, 1-epsilon)\n",
    "    N, M = pred.shape\n",
    "    y_onehot = np.zeros((N, M))\n",
    "    for i, val in enumerate(target):\n",
    "        y_onehot[i, val] = 1\n",
    "    logLoss = -1/N * np.sum(y_onehot*np.log(clip))\n",
    "    return logLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1997dabe-7328-4820-baa1-cb0598d58b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6090, 9229), (1523, 9229))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
    "                     analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                     ngram_range=(1,3), use_idf=1, smooth_idf=1,\n",
    "                     sublinear_tf=1, stop_words='english')\n",
    "\n",
    "tfv.fit(x_train + x_val) # semi-supervised learning\n",
    "xtrain_tfv = tfv.transform(x_train)\n",
    "xval_tfv = tfv.transform(x_val)\n",
    "xtrain_tfv.shape, xval_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5cddf66-60bb-4b52-9077-edc9d9ea6ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss = 0.5041657186736527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, y_train)\n",
    "pred_tfv = clf.predict_proba(xval_tfv)\n",
    "print(f\"LogLoss = {LogLoss(y_val, pred_tfv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d13113e-f510-40ee-8c71-1dafae296c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss = 0.4827469320349906\n"
     ]
    }
   ],
   "source": [
    "ctv = CountVectorizer(analyzer=\"word\", token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1,3), stop_words=\"english\")\n",
    "ctv.fit(x_train+x_val)\n",
    "xtrain_ctv = ctv.transform(x_train)\n",
    "xval_ctv = ctv.transform(x_val)\n",
    "\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, y_train)\n",
    "pred_ctv = clf.predict_proba(xval_ctv)\n",
    "print(f\"LogLoss = {LogLoss(y_val, pred_ctv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6bafe-f955-490c-85ed-b2996517cd08",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "286adfb9-877c-492f-8312-dc05a616d931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss = 0.4996628081202825\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, \n",
    "                        colsample_bytree=0.8, subsample=0.8,\n",
    "                        nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv, y_train)\n",
    "pred_xg = clf.predict_proba(xval_tfv)\n",
    "print(f\"LogLoss = {metrics.log_loss(y_val, pred_xg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11827db-51df-4be4-a2ec-d2ae8428c971",
   "metadata": {},
   "source": [
    "## Grid Search with SVD and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5b23709c-ece7-470a-b159-09700fad320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99953f47-e66f-4c08-83cf-43093396ff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best score: 0.5079954924102028\n",
      "Best parameters set: \n",
      "lr__C : 1.0\n",
      "svd__n_components : 180\n",
      "[CV 2/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 2/2; 1/6] END .............nb__alpha=0.001;, score=-1.196 total time=   0.0s\n",
      "[CV 1/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 1/2; 5/6] END ................nb__alpha=10;, score=-0.609 total time=   0.0s\n",
      "[CV 1/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 1/2; 6/6] END ...............nb__alpha=100;, score=-0.672 total time=   0.0s\n",
      "[CV 2/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 2/2; 1/6] END .............nb__alpha=0.001;, score=-1.196 total time=   0.0s\n",
      "[CV 2/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 2/2; 5/6] END ................nb__alpha=10;, score=-0.611 total time=   0.0s\n",
      "[CV 2/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 2/2; 3/6] END ...............nb__alpha=0.1;, score=-0.579 total time=   0.0s\n",
      "[CV 2/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 2/2; 1/6] END .............nb__alpha=0.001;, score=-1.196 total time=   0.0s\n",
      "[CV 2/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 2/2; 5/6] END ................nb__alpha=10;, score=-0.611 total time=   0.0s\n",
      "[CV 2/2; 3/6] START lr__C=1.0, svd__n_components=120............................\n",
      "[CV 2/2; 3/6] END lr__C=1.0, svd__n_components=120;, score=-0.534 total time=   4.3s\n",
      "[CV 1/2; 6/6] START lr__C=10, svd__n_components=180.............................\n",
      "[CV 1/2; 6/6] END lr__C=10, svd__n_components=180;, score=-0.505 total time=   2.8s\n",
      "[CV 2/2; 4/6] START lr__C=1.0, svd__n_components=180............................\n",
      "[CV 2/2; 4/6] END lr__C=1.0, svd__n_components=180;, score=-0.509 total time=   6.3s\n",
      "[CV 1/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 1/2; 1/6] END .............nb__alpha=0.001;, score=-1.059 total time=   0.0s\n",
      "[CV 2/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 2/2; 5/6] END ................nb__alpha=10;, score=-0.611 total time=   0.0s\n",
      "[CV 1/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 1/2; 1/6] END .............nb__alpha=0.001;, score=-1.059 total time=   0.0s\n",
      "[CV 1/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 1/2; 5/6] END ................nb__alpha=10;, score=-0.609 total time=   0.0s\n",
      "[CV 1/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 1/2; 3/6] END ...............nb__alpha=0.1;, score=-0.544 total time=   0.0s\n",
      "[CV 2/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 2/2; 6/6] END ...............nb__alpha=100;, score=-0.673 total time=   0.0s\n",
      "[CV 2/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 2/2; 4/6] END .................nb__alpha=1;, score=-0.494 total time=   0.0s\n",
      "[CV 2/2; 2/6] START lr__C=0.1, svd__n_components=180............................\n",
      "[CV 2/2; 2/6] END lr__C=0.1, svd__n_components=180;, score=-0.512 total time=   6.2s\n",
      "[CV 2/2; 2/6] START lr__C=0.1, svd__n_components=180............................\n",
      "[CV 2/2; 2/6] END lr__C=0.1, svd__n_components=180;, score=-0.514 total time=   6.3s\n",
      "[CV 2/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 2/2; 3/6] END ...............nb__alpha=0.1;, score=-0.579 total time=   0.0s\n",
      "[CV 1/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 1/2; 3/6] END ...............nb__alpha=0.1;, score=-0.544 total time=   0.0s\n",
      "[CV 1/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 1/2; 1/6] END .............nb__alpha=0.001;, score=-1.059 total time=   0.0s\n",
      "[CV 1/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 1/2; 1/6] END .............nb__alpha=0.001;, score=-1.059 total time=   0.0s\n",
      "[CV 1/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 1/2; 5/6] END ................nb__alpha=10;, score=-0.609 total time=   0.0s\n",
      "[CV 1/2; 3/6] START lr__C=1.0, svd__n_components=120............................\n",
      "[CV 1/2; 3/6] END lr__C=1.0, svd__n_components=120;, score=-0.512 total time=   4.3s\n",
      "[CV 2/2; 6/6] START lr__C=10, svd__n_components=180.............................\n",
      "[CV 2/2; 6/6] END lr__C=10, svd__n_components=180;, score=-0.508 total time=   2.8s\n",
      "[CV 1/2; 4/6] START lr__C=1.0, svd__n_components=180............................\n",
      "[CV 1/2; 4/6] END lr__C=1.0, svd__n_components=180;, score=-0.507 total time=   6.3s\n",
      "[CV 2/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 2/2; 2/6] END ..............nb__alpha=0.01;, score=-0.855 total time=   0.0s\n",
      "[CV 2/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 2/2; 2/6] END ..............nb__alpha=0.01;, score=-0.855 total time=   0.0s\n",
      "[CV 2/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 2/2; 6/6] END ...............nb__alpha=100;, score=-0.673 total time=   0.0s\n",
      "[CV 2/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 2/2; 4/6] END .................nb__alpha=1;, score=-0.494 total time=   0.0s\n",
      "[CV 2/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 2/2; 2/6] END ..............nb__alpha=0.01;, score=-0.855 total time=   0.0s\n",
      "[CV 2/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 2/2; 6/6] END ...............nb__alpha=100;, score=-0.673 total time=   0.0s\n",
      "[CV 2/2; 4/6] START lr__C=1.0, svd__n_components=180............................\n",
      "[CV 2/2; 4/6] END lr__C=1.0, svd__n_components=180;, score=-0.507 total time=   6.1s\n",
      "[CV 1/2; 2/6] START lr__C=0.1, svd__n_components=180............................\n",
      "[CV 1/2; 2/6] END lr__C=0.1, svd__n_components=180;, score=-0.506 total time=   6.5s\n",
      "[CV 1/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 1/2; 2/6] END ..............nb__alpha=0.01;, score=-0.771 total time=   0.0s\n",
      "[CV 2/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 2/2; 6/6] END ...............nb__alpha=100;, score=-0.673 total time=   0.0s\n",
      "[CV 1/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 1/2; 2/6] END ..............nb__alpha=0.01;, score=-0.771 total time=   0.0s\n",
      "[CV 1/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 1/2; 6/6] END ...............nb__alpha=100;, score=-0.672 total time=   0.0s\n",
      "[CV 1/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 1/2; 4/6] END .................nb__alpha=1;, score=-0.487 total time=   0.0s\n",
      "[CV 1/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 1/2; 2/6] END ..............nb__alpha=0.01;, score=-0.771 total time=   0.0s\n",
      "[CV 1/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 1/2; 6/6] END ...............nb__alpha=100;, score=-0.672 total time=   0.0s\n",
      "[CV 1/2; 4/6] START lr__C=1.0, svd__n_components=180............................\n",
      "[CV 1/2; 4/6] END lr__C=1.0, svd__n_components=180;, score=-0.512 total time=   6.1s\n",
      "[CV 2/2; 1/6] START lr__C=0.1, svd__n_components=120............................\n",
      "[CV 2/2; 1/6] END lr__C=0.1, svd__n_components=120;, score=-0.523 total time=   4.1s\n",
      "[CV 1/2; 5/6] START lr__C=10, svd__n_components=120.............................\n",
      "[CV 1/2; 5/6] END lr__C=10, svd__n_components=120;, score=-0.520 total time=   2.9s\n",
      "[CV 1/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 1/2; 4/6] END .................nb__alpha=1;, score=-0.487 total time=   0.0s\n",
      "[CV 1/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 1/2; 4/6] END .................nb__alpha=1;, score=-0.487 total time=   0.0s\n",
      "[CV 1/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 1/2; 2/6] END ..............nb__alpha=0.01;, score=-0.771 total time=   0.0s\n",
      "[CV 1/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 1/2; 5/6] END ................nb__alpha=10;, score=-0.609 total time=   0.0s\n",
      "[CV 1/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 1/2; 3/6] END ...............nb__alpha=0.1;, score=-0.544 total time=   0.0s\n",
      "[CV 1/2; 1/6] START lr__C=0.1, svd__n_components=120............................\n",
      "[CV 1/2; 1/6] END lr__C=0.1, svd__n_components=120;, score=-0.521 total time=   3.9s\n",
      "[CV 1/2; 5/6] START lr__C=10, svd__n_components=120.............................\n",
      "[CV 1/2; 5/6] END lr__C=10, svd__n_components=120;, score=-0.525 total time=   2.9s\n",
      "[CV 2/2; 3/6] START lr__C=1.0, svd__n_components=120............................\n",
      "[CV 2/2; 3/6] END lr__C=1.0, svd__n_components=120;, score=-0.528 total time=   4.1s\n",
      "[CV 2/2; 5/6] START lr__C=10, svd__n_components=120.............................\n",
      "[CV 2/2; 5/6] END lr__C=10, svd__n_components=120;, score=-0.533 total time=   2.9s\n",
      "[CV 2/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 2/2; 4/6] END .................nb__alpha=1;, score=-0.494 total time=   0.0s\n",
      "[CV 2/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 2/2; 4/6] END .................nb__alpha=1;, score=-0.494 total time=   0.0s\n",
      "[CV 2/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 2/2; 2/6] END ..............nb__alpha=0.01;, score=-0.855 total time=   0.0s\n",
      "[CV 1/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 1/2; 6/6] END ...............nb__alpha=100;, score=-0.672 total time=   0.0s\n",
      "[CV 2/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 2/2; 3/6] END ...............nb__alpha=0.1;, score=-0.579 total time=   0.0s\n",
      "[CV 2/2; 1/6] START lr__C=0.1, svd__n_components=120............................\n",
      "[CV 2/2; 1/6] END lr__C=0.1, svd__n_components=120;, score=-0.526 total time=   3.9s\n",
      "[CV 2/2; 5/6] START lr__C=10, svd__n_components=120.............................\n",
      "[CV 2/2; 5/6] END lr__C=10, svd__n_components=120;, score=-0.524 total time=   2.8s\n",
      "[CV 1/2; 3/6] START lr__C=1.0, svd__n_components=120............................\n",
      "[CV 1/2; 3/6] END lr__C=1.0, svd__n_components=120;, score=-0.517 total time=   4.1s\n",
      "[CV 1/2; 6/6] START lr__C=10, svd__n_components=180.............................\n",
      "[CV 1/2; 6/6] END lr__C=10, svd__n_components=180;, score=-0.515 total time=   3.3s\n",
      "[CV 1/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 1/2; 3/6] END ...............nb__alpha=0.1;, score=-0.544 total time=   0.0s\n",
      "[CV 2/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 2/2; 3/6] END ...............nb__alpha=0.1;, score=-0.579 total time=   0.0s\n",
      "[CV 2/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 2/2; 1/6] END .............nb__alpha=0.001;, score=-1.196 total time=   0.0s\n",
      "[CV 2/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 2/2; 5/6] END ................nb__alpha=10;, score=-0.611 total time=   0.0s\n",
      "[CV 1/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 1/2; 4/6] END .................nb__alpha=1;, score=-0.487 total time=   0.0s\n",
      "[CV 1/2; 2/6] START lr__C=0.1, svd__n_components=180............................\n",
      "[CV 1/2; 2/6] END lr__C=0.1, svd__n_components=180;, score=-0.506 total time=   6.0s\n",
      "[CV 1/2; 1/6] START lr__C=0.1, svd__n_components=120............................\n",
      "[CV 1/2; 1/6] END lr__C=0.1, svd__n_components=120;, score=-0.522 total time=   4.5s\n",
      "[CV 2/2; 6/6] START lr__C=10, svd__n_components=180.............................\n",
      "[CV 2/2; 6/6] END lr__C=10, svd__n_components=180;, score=-0.507 total time=   2.9s\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD()\n",
    "scl = StandardScaler()\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "        ('svd', svd),\n",
    "        ('slc', scl),\n",
    "        ('lr', lr_model),\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    'svd__n_components' : [120, 180],\n",
    "    'lr__C' : [0.1, 1.0, 10],\n",
    "}\n",
    "\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer, verbose=10, n_jobs=-1, refit=True, cv=2)\n",
    "model.fit(xtrain_tfv, y_train)\n",
    "\n",
    "print(f\"Best score: {-model.best_score_}\")\n",
    "print(\"Best parameters set: \")\n",
    "best_params = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(f\"{param_name} : {best_params[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a999e4ba-5599-49a3-8610-c78a4c36398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best score: 0.49063318537754314\n",
      "Best parameters set: \n",
      "nb__alpha : 1\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "        ('nb', nb_model),\n",
    "    ])\n",
    "\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer, verbose=10, n_jobs=-1, refit=True, cv=2)\n",
    "model.fit(xtrain_tfv, y_train)\n",
    "\n",
    "print(f\"Best score: {-model.best_score_}\")\n",
    "print(\"Best parameters set: \")\n",
    "best_params = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(f\"{param_name} : {best_params[param_name]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37c013-7f75-4e27-b49e-39f40359adad",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "533e522d-3f8e-451f-a193-ce87efa2e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a9d86951-38c1-4ab8-a537-868a6d959823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.043289899826049805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 29,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6db56f90224b3e80bdaaeab5e69a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04447627067565918,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 570,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45e311d2f8240d3b31d8726968727ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04552602767944336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 213450,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38845d8393ff423bac31d595cb66d1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020629405975341797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 435797,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae21eb6b95094b7db9a79a63ecc67eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8ab47698-3fb8-4fb1-a34a-a0112ea43db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-80461f5a0f47ecd9\n",
      "Found cached dataset csv (/home/hainam/.cache/huggingface/datasets/csv/default-80461f5a0f47ecd9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04813098907470703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1534cdfd8140af8bfab32a682406ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2842d9be0ed29a04\n",
      "Found cached dataset csv (/home/hainam/.cache/huggingface/datasets/csv/default-2842d9be0ed29a04/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026799678802490234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4245656fa83402fa1828062e755656f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_dataset = load_dataset(\"csv\", data_files=train_path)\n",
    "test_dataset = load_dataset(\"csv\", data_files=test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fe4efc85-a747-4687-99d6-14ae680400fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [1, 4, 5, 6, 7],\n",
       " 'keyword': [None, None, None, None, None],\n",
       " 'location': [None, None, None, None, None],\n",
       " 'text': ['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n",
       "  'Forest fire near La Ronge Sask. Canada',\n",
       "  \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\",\n",
       "  '13,000 people receive #wildfires evacuation orders in California ',\n",
       "  'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school '],\n",
       " 'target': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "be453339-e7cb-4744-9c6f-62dfb4e584f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03436636924743652,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 8,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f820aa7a884481a824ef2333ea1829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "08c44137-6e5c-4d77-b2dd-a841495e9228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74dd138-fd86-4ef8-a0a2-dccbc004d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f78a0a-1f07-483f-8602-e2a28ed74eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
