{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f5fa401-f160-4e3f-97ef-9aaee9df31b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import pipeline, metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec116b-b62e-428f-a672-87c13cfe1ac7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5bf15be-2fb7-4a53-adca-33f33432e806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length : 7613\n",
      "Test length : 3263\n"
     ]
    }
   ],
   "source": [
    "sample_path = \"./data/sample_submission.csv\"\n",
    "train_path = \"./data/train.csv\"\n",
    "test_path = \"./data/test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_sample = pd.read_csv(sample_path)\n",
    "\n",
    "print(f\"Train length : {len(df_train)}\")\n",
    "print(f\"Test length : {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1516b40-60cd-4947-b870-8e5e5ad6c297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32c94576-e491-48d4-a5df-021b36a21100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Birmingham', 'Est. September 2012 - Bristol', ...,\n",
       "       'Vancouver, Canada', 'London ', 'Lincoln'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10387571-54d3-4654-b19c-09a48a5586f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 1523)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2)\n",
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d381beb1-2642-4c40-b602-3a8e5a4839a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9efe0e91-562e-4b09-b358-130430e9b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train = list(df_train.target), list(df_train.text)\n",
    "y_val, x_val = list(df_val.target), list(df_val.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246ece0-77fc-44d5-b162-ba7a2daea469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TFIDF and CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adae7917-034d-400d-9346-dbe9a02702af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLoss(target, pred, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Use sklearn.metrics.log_loss built in function instead\n",
    "    \"\"\"\n",
    "    clip = np.clip(pred, epsilon, 1-epsilon)\n",
    "    N, M = pred.shape\n",
    "    y_onehot = np.zeros((N, M))\n",
    "    for i, val in enumerate(target):\n",
    "        y_onehot[i, val] = 1\n",
    "    logLoss = -1/N * np.sum(y_onehot*np.log(clip))\n",
    "    return logLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1997dabe-7328-4820-baa1-cb0598d58b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6090, 9229), (1523, 9229))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
    "                     analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                     ngram_range=(1,3), use_idf=1, smooth_idf=1,\n",
    "                     sublinear_tf=1, stop_words='english')\n",
    "\n",
    "tfv.fit(x_train + x_val) # semi-supervised learning\n",
    "xtrain_tfv = tfv.transform(x_train)\n",
    "xval_tfv = tfv.transform(x_val)\n",
    "xtrain_tfv.shape, xval_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5cddf66-60bb-4b52-9077-edc9d9ea6ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss = 0.49150263459876953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, y_train)\n",
    "pred_tfv = clf.predict_proba(xval_tfv)\n",
    "print(f\"LogLoss = {LogLoss(y_val, pred_tfv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d13113e-f510-40ee-8c71-1dafae296c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss = 0.46888304010401244\n"
     ]
    }
   ],
   "source": [
    "ctv = CountVectorizer(analyzer=\"word\", token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1,3), stop_words=\"english\")\n",
    "ctv.fit(x_train+x_val)\n",
    "xtrain_ctv = ctv.transform(x_train)\n",
    "xval_ctv = ctv.transform(x_val)\n",
    "\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, y_train)\n",
    "pred_ctv = clf.predict_proba(xval_ctv)\n",
    "print(f\"LogLoss = {LogLoss(y_val, pred_ctv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6bafe-f955-490c-85ed-b2996517cd08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "286adfb9-877c-492f-8312-dc05a616d931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss = 0.5015601728955994\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, \n",
    "                        colsample_bytree=0.8, subsample=0.8,\n",
    "                        nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv, y_train)\n",
    "pred_xg = clf.predict_proba(xval_tfv)\n",
    "print(f\"LogLoss = {metrics.log_loss(y_val, pred_xg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11827db-51df-4be4-a2ec-d2ae8428c971",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Grid Search with SVD and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b23709c-ece7-470a-b159-09700fad320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99953f47-e66f-4c08-83cf-43093396ff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best score: 0.5101428326321005\n",
      "Best parameters set: \n",
      "lr__C : 0.1\n",
      "svd__n_components : 180\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD()\n",
    "scl = StandardScaler()\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "        ('svd', svd),\n",
    "        ('slc', scl),\n",
    "        ('lr', lr_model),\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    'svd__n_components' : [120, 180],\n",
    "    'lr__C' : [0.1, 1.0, 10],\n",
    "}\n",
    "\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer, verbose=10, n_jobs=-1, refit=True, cv=2)\n",
    "model.fit(xtrain_tfv, y_train)\n",
    "\n",
    "print(f\"Best score: {-model.best_score_}\")\n",
    "print(\"Best parameters set: \")\n",
    "best_params = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(f\"{param_name} : {best_params[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a999e4ba-5599-49a3-8610-c78a4c36398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best score: 0.4915541852700148\n",
      "Best parameters set: \n",
      "nb__alpha : 1\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "        ('nb', nb_model),\n",
    "    ])\n",
    "\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer, verbose=10, n_jobs=-1, refit=True, cv=2)\n",
    "model.fit(xtrain_tfv, y_train)\n",
    "\n",
    "print(f\"Best score: {-model.best_score_}\")\n",
    "print(\"Best parameters set: \")\n",
    "best_params = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(f\"{param_name} : {best_params[param_name]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c16e2d-0281-4e60-bd00-3dc63db62530",
   "metadata": {},
   "source": [
    "## BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0459a6f1-b4e2-4f52-a821-2961b06878b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "543337d4-e590-4d74-9f5f-a1ddd3890d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerBase.encode_plus of PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4b10828-3831-4896-8794-c6c422edbff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb25e110-94c8-4a38-9f12-0d2fb9c2658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text): \n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e569baa8-297b-440b-8f0b-027c5984a3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 381/381 [05:45<00:00,  1.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [01:27<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 42s, sys: 2.22 s, total: 28min 44s\n",
      "Wall time: 7min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "batch_size = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "val_dataloader = torch.utils.data.DataLoader(x_val, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "N, M = len(x_train), len(x_val)\n",
    "\n",
    "def get_bert_embedding(dataloader, size):\n",
    "    x_bert = []\n",
    "    for batch_texts in tqdm(dataloader):\n",
    "        batch_texts = np.array(batch_texts)\n",
    "        x_bert.extend(list(map(process, batch_texts)))\n",
    "    x_bert = torch.cat(x_bert, axis=0)\n",
    "    x_bert = x_bert.reshape(size, -1)\n",
    "    return x_bert\n",
    "\n",
    "xtrain_bert = get_bert_embedding(train_dataloader, N)\n",
    "ytrain_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "\n",
    "xval_bert = get_bert_embedding(val_dataloader, M)\n",
    "yval_tensor = torch.tensor(y_val, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8b7d8176-eb2d-4f1d-a5dc-f75efae0f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 512\n",
    "num_epoch = 2\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "class ClumsyDataset(Dataset):\n",
    "    def __init__(self, text, target):\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx], self.target[idx]\n",
    "\n",
    "class Clumsy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Clumsy, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(768, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(300, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(300, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 2),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "clumsyModel = Clumsy().to(device)\n",
    "clumsyTrainDataset = ClumsyDataset(xtrain_bert, ytrain_tensor)\n",
    "clumsyValDataset = ClumsyDataset(xval_bert, yval_tensor)\n",
    "train_loader = DataLoader(clumsyTrainDataset)\n",
    "val_loader = DataLoader(clumsyValDataset)\n",
    "\n",
    "optimizer = torch.optim.Adam(clumsyModel.parameters(), lr=learning_rate, weight_decay=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ca739a8c-5c98-4e77-9fc0-dceb2f219538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/6090 (0%)]\tLoss: 0.699023\n",
      "Train Epoch: 0 [1000/6090 (16%)]\tLoss: 0.664470\n",
      "Train Epoch: 0 [2000/6090 (33%)]\tLoss: 0.576411\n",
      "Train Epoch: 0 [3000/6090 (49%)]\tLoss: 0.644753\n",
      "Train Epoch: 0 [4000/6090 (66%)]\tLoss: 0.749572\n",
      "Train Epoch: 0 [5000/6090 (82%)]\tLoss: 0.783478\n",
      "Train Epoch: 0 [6000/6090 (99%)]\tLoss: 0.779244\n",
      "Train Epoch: 1 [0/6090 (0%)]\tLoss: 0.731718\n",
      "Train Epoch: 1 [1000/6090 (16%)]\tLoss: 0.664617\n",
      "Train Epoch: 1 [2000/6090 (33%)]\tLoss: 0.576617\n",
      "Train Epoch: 1 [3000/6090 (49%)]\tLoss: 0.644643\n",
      "Train Epoch: 1 [4000/6090 (66%)]\tLoss: 0.749732\n",
      "Train Epoch: 1 [5000/6090 (82%)]\tLoss: 0.783475\n",
      "Train Epoch: 1 [6000/6090 (99%)]\tLoss: 0.779510\n"
     ]
    }
   ],
   "source": [
    "clumsyModel.train()\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_idx, (inp, target) in enumerate(train_loader):\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = clumsyModel(inp)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1000 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(inp), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1a272a52-1bfb-459d-9f8b-2b596b8f1f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7013\n"
     ]
    }
   ],
   "source": [
    "clumsyModel.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inp, target) in enumerate(val_loader):\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "        out = clumsyModel(inp)\n",
    "        loss = criterion(out, target)\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(val_loader)        \n",
    "print('\\nTest set: Average loss: {:.4f}'.format(\n",
    "        test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ce1b2-f094-42a3-b0d1-fe0db2124897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4c155-340a-4e6e-85c8-7558f2ef5b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394e221-3490-456c-9d44-3755bc30efb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf10073-b16e-4fd2-955b-5d6914e1502e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1f383-8d75-4c1b-aefb-5f0e82175adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba0d6e-d075-4539-8420-4ee44dc8afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# import multiprocessing\n",
    "# import time\n",
    "\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# def process(i):\n",
    "#     return i ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98e1d8-a311-4eb0-88c1-e36a9d7aa549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time    \n",
    "# results = Parallel(n_jobs=3, verbose=5)(delayed(process)(i) for i in range(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216557ec-cfd9-4eb0-b338-3d9b177009bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# num = 100000\n",
    "# results = [process(i) for i in range(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be9a0fd-bb1b-4045-ba3b-a3566b9ad164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# import numpy as np\n",
    "# arr = np.array(range(num))\n",
    "# results = list(map(process, arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
